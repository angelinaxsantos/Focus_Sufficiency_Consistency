# ğŸ“˜ Focus, Sufficiency and Consistency in XAI

Este repositÃ³rio contÃ©m o trabalho desenvolvido para avaliar **a fidelidade de mÃ©todos de InteligÃªncia Artificial ExplicÃ¡vel (XAI)**, combinando duas abordagens complementares da literatura recente.

O estudo segue:
- **Focus score** â€” *Arias et al., 2022*, aplicado a **classificaÃ§Ã£o de imagens**, avaliando se a relevÃ¢ncia das explicaÃ§Ãµes estÃ¡ concentrada nas regiÃµes corretas do input.
- **Consistency, Sufficiency e Uniqueness** â€” *Dasgupta et al., 2022*, aplicadas a **dados tabulares**, para medir a fidelidade das explicaÃ§Ãµes em relaÃ§Ã£o ao comportamento real do modelo.

---

## ğŸ”¬ Experimentos realizados

### ğŸ–¼ï¸ Focus Score (Imagens)
- **Modelo**: CNN para classificaÃ§Ã£o de 7 tipos de ecopontos  
- **MÃ©todos XAI**:
  - GradCAM
  - GradCAM++
  - LIME
  - SmoothGrad
  - LRP-like
  - Integrated Gradients  
- **AvaliaÃ§Ã£o**: mosaicos 2Ã—2 (in-distribution noise)

### ğŸ“Š MÃ©tricas de Fidelidade (Dataset Adult)
- Decision Tree (caminhos raiz â†’ folha)
- Logistic Regression + Top-k coefficients
- Random Forest + SHAP
- Random Forest + LIME
- Random Forest + Anchors (vÃ¡rios thresholds)

---

## ğŸ¯ Objetivo

Avaliar **quantitativamente a fidelidade das explicaÃ§Ãµes**, indo alÃ©m da plausibilidade visual, analisando:
- **Consistency**: se explicaÃ§Ãµes idÃªnticas levam Ã  mesma prediÃ§Ã£o
- **Sufficiency**: se a explicaÃ§Ã£o Ã© suficiente para garantir a decisÃ£o
- **Uniqueness**: diversidade das explicaÃ§Ãµes geradas

---

## ğŸ§  Principais conclusÃµes

- **GradCAM e LIME** apresentaram maior fidelidade em imagens segundo o Focus score
- **Decision Trees e Anchors** produzem explicaÃ§Ãµes altamente consistentes
- **LIME** mostrou bom equilÃ­brio entre fidelidade e interpretabilidade em dados tabulares
- **SHAP discretizado top-k** apresentou limitaÃ§Ãµes em datasets tabulares densos
- A escolha do mÃ©todo XAI depende do **tipo de dado, modelo e objetivo da anÃ¡lise**

---

## ğŸ“š ReferÃªncias

- Arias et al., *FOCUS: Faithful Explanations for CNNs*, 2022  
- Dasgupta et al., *Evaluating Faithfulness of Explanations*, 2022  

---

Este repositÃ³rio acompanha integralmente o relatÃ³rio e tem como objetivo apoiar estudos sobre **avaliaÃ§Ã£o rigorosa de explicabilidade em InteligÃªncia Artificial**.
